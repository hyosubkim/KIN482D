{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b9873b-0150-4515-a902-b94b714a44c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KIN 482D: Computational modeling of human sensorimotor control and learning\n",
    "\n",
    "## Chapter 4: The response distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64581bde-bdad-4e89-8060-84eb9b0c86eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housekeeping\n",
    "\n",
    "- Make sure you have someone you can consult and pair program with if you are new or a little rusty with Python (also, come speak with me if you're concerned)\n",
    "- Problem Set 3 will be time consuming, by necessity&mdash;fitting models to data takes lots of time, even when you know how! \n",
    "- Broken record time: Get started early, and have fun!\n",
    "- In two weeks, you will learn one of the most popular (and useful) Bayesian models ever, and it will likely feel like a breeze\n",
    "- Start thinking about final projects \n",
    "    - Novel modeling idea (must be Bayesian)\n",
    "    - Take a challenging, influential paper (I can recommend) and create a combined tutorial/simulation based on model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11b919-7159-4e75-bfde-22b2bd5b220c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big question in Chapter 4\n",
    "\n",
    "*How does a Bayesian model predict a human observer's responses on a perceptual task?*\n",
    "\n",
    "A model formalizes our understanding of a behavior. If our understanding is correct, it must be able to make accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d9fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "\n",
    "- Discuss why, upon repeated presentations of the same stimulus, a Bayesian observer’s posterior mean estimate (i.e., the observer’s response) is a random variable\n",
    "\n",
    "- Derive probability distribution of the Bayesian observer’s responses (Step 3 of Bayesian modeling)\n",
    "\n",
    "- The derived response distribution allows researchers to compare the predictions of the Bayesian model to the observer’s actual behaviour in the context of a psychophysical experiment\n",
    "\n",
    "- Discuss bias and variance of the posterior mean estimate (PME) and compare to the maximum-likelihood estimate (MLE)\n",
    "\n",
    "- Discuss optimality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df166f-fe5b-4795-971c-b183d6debd4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inherited variability\n",
    "\n",
    "- To compare our model with observer's behavior, we must specify what the model predicts for observer's responses when true stim is $s$\n",
    "\n",
    "- In other words, we must derive $p(\\hat{s}|s)$ - the \"estimate distribution\" or \"response distribution\"\n",
    "\n",
    "- Because $x_\\text{obs}$ is a RV for a given $s$, so is the stimulus estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c14e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Inherited variability\": Likelihood function, posterior, and PME are not fixed!\n",
    "\n",
    "&nbsp;\n",
    "<img src=\"images/fig4-1.png\" width=500>\n",
    "\n",
    "- Remember: The posterior probability is realized on a single trial\n",
    "- Distributions move around on a trial-by-trial basis, even when $s$ is constant, because $x_{\\text{obs}}$ is a RV (why we need lots of trials in our experiments) \n",
    "- Stochasticity in the stimulus estimate, or the subject’s response, is “inherited from” the stochasticity in the measurement.\n",
    "- In figure above, note relationship between $s_n$ and $x_{\\text{obs}_n}. They will usually not be aligned because of measurement noise/variability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e96f6f-6749-4cc4-8271-910a3f54dc08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Response Distribution\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mu_\\text{post} &= \\dfrac{Jx_\\text{obs} + J_s\\mu}{J + J_s} \\\\\n",
    "\\hat{s}_\\text{PM} &= \\mu_\\text{post} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "That is, on a given trial, the stimulus estimate is the posterior mean.\n",
    "\n",
    "The rest goes up on the board, including \"Properties of Linear Combinations of Random Variables\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe2645",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on notation\n",
    "\n",
    "- **Expected value** is a fancy term for the average, or mean\n",
    "- Precision notation: $J = \\dfrac{1}{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4c92d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First 3 steps of Bayesian modeling\n",
    "\n",
    "<img src=\"images/table4-1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c011338-472f-4bad-9cb7-3a58ba894a1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Belief versus Response Distributions\n",
    "\n",
    "- Priors, normalized likelihoods, and posteriors represent degree of belief observer has in different hypothesized world states\n",
    "    - Beliefs are defined on each individual trial, internal to observer, and not directly measurable (they are estimated based on assumptions of our model)\n",
    "- Response distribution is a summary of observer's behavior aacross many trials\n",
    "    - Directly measurable and exists even if observer is not Bayesian    \n",
    "- Difference manifests in their variances (see next figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042343e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First 3 steps of Bayesian modeling\n",
    "\n",
    "<img src=\"images/table3-1.png\" width=600>\n",
    "<img src=\"images/table4-2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da510f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance of belief versus response distributions\n",
    "\n",
    "<img src=\"images/fig4-2.png\" width=500>\n",
    "\n",
    "Exercise 4.2 asks you why SD of response distribution eventually starts to decrease. Hint: Remember that we are assuming $s$ is held constant; therefore, what contributes to variability of $\\hat{s}$ across trials? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73f3ae-28d7-40ae-803c-fa9ac60d57d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 Maximum-likelihood estimate\n",
    "\n",
    "- $\\hat{s}_{\\text{ML}} = x_{\\text{obs}}$\n",
    "- Distribution of MLE of s is equivalent to measurement distribution: $N(s, \\sigma^2)$\n",
    "- MLE ignores stimulus (prior) distribution\n",
    "- Just as we studied distribution of $\\hat{s}_\\text{PM}$ for given $s$, we can study distribution of MLE, $\\hat{s}_\\text{ML}$, for given $s$\n",
    "- We already know this latter distribution&mdash;what is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7e148-d2dc-4d1c-880e-9a49842a05c6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/fig4-3_tophalf.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe0f08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 Bias and mean squared error\n",
    "\n",
    "- **The posterior mean is biased from the stimulus towards the mean of the prior.**\n",
    "\n",
    "- The *bias* of an estimate $\\hat{s}$ is defined as the difference between the average estimate and the true stimulus:\n",
    "\n",
    "$$ Bias[\\hat{s}|s] \\equiv \\mathbb{E}[\\hat{s}|s] - s$$\n",
    "\n",
    "- As the posterior mean estimate is biased, so how can it be \"optimal\" as is frequently claimed about Bayesian models? \n",
    "\n",
    "- Turns out that PME is good in the sense that it minimizes the overall *mean squared error* between the estimate and the true stimulus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0fd75-0992-49d9-bdca-b8298aab6aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-Variance Decomposition of MSE\n",
    "\n",
    "Go over on board. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d381091-d32c-4f15-8b86-3cbd90dbd7f6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/fig4-3.png\" width=800>\n",
    "Same Mean Squared Error can arise from different combinations of bias versus variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae521c50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More comparisons of PME vs MLE and why PME is optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09904e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/fig4-3.png\" width=800>\n",
    "\n",
    "**Figure 4.3:** Comparison between the posterior mean estimate (PME) and the maximum-likelihood\n",
    "estimate (MLE). In this example, the stimulus distribution has $\\mu = 0$ and $\\sigma_s = 8$. (A) Scatterplots\n",
    "of PMEs and MLEs against the true stimulus. Dashed lines indicate the expected values. The\n",
    "larger the noise, the lower the slope of the expected value of the PME. (B) Mean squared error as\n",
    "a function of the stimulus for the PMEs and MLEs. Mean squared error (solid lines) is the sum\n",
    "of squared bias and variance. Although the PME is biased, its variance (dashed light blue line) is\n",
    "lower than that of the MLE (green line). The stimuli that occur often according to the stimulus\n",
    "distribution (shading indicates probability) are such that the overall (stimulus-averaged) MSE of\n",
    "the PME (light blue number, in parentheses) is always lower than that of the MLE (green number)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb65716-9731-4df0-a08f-bc386dc8408b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The world according to E.T. Jaynes (possibly the G.O.A.T. of Bayesian statistics)\n",
    "\n",
    "<img src=\"images/ETJaynes2.jpg\">\n",
    "\n",
    "“When we call the quantity...‘bias’, that makes it sound like something awfully reprehensible, which we must get rid of at all costs. If it had been called instead the ‘component of error orthogonal to the variance’,...it would have been clear to all that these two contributions to the error are on an equal footing...This is just the price one pays for choosing a technical terminology that carries an emotional load, implying value judgments...” (Jaynes, 2003, p. 514)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e816ad-34d1-4fb1-88e3-33d506834d2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief aside about response noise\n",
    "\n",
    "- We have assumed observer's response is equal to stimulus estimate\n",
    "- In reality, response (e.g., motor) noise could exist (e.g., accuracy in choosing a response with cursor, reach to inferred target location, etc.)\n",
    "- We will see in a few weeks how $\\sigma^2_\\text{motor}$ can be incorporated into model (not difficult, but adds extra parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfd44b-6ad1-4093-b072-c09c22119908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reflections on Bayesian models\n",
    "\n",
    "- This model is representative of Bayesian modeling in general\n",
    "- Essence of Bayesian *ideal observer* is that they consider all possible values of world state, and compute probabilities of those values (most non-Bayesian models work with only point estimates)\n",
    "- A great advantage of Bayesian modeling is that you can build a complete model of a psychophysical task before collecting any data\n",
    "    - Model specifies how observer should do task in order to be optimal\n",
    "    - Bayesian models are also called *normative* models, as they set the norm/standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75f805",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Bayesian modeling consists of three steps: defining the generative model, deriving an expression for the observer’s posterior mean estimate (PME), and deriving the distribution of the posterior mean estimate over many trials.\n",
    "- The mean squared error (MSE) as a measure of the performance of an estimate. We distinguished stimulus-conditioned MSE and overall MSE.\n",
    "- Stimulus-conditioned MSE is a sum of squared bias and variance.\n",
    "- We compared the distribution of the PME to that of the MLE. Although the latter is unbiased, it has higher variance for frequently occurring stimuli, and it is worse overall as a result.\n",
    "- It is easy to confuse the functions and distributions in the different steps, as they look similar (in this chapter, they are all Gaussian). They must be distinguished carefully, as we did in Table 4.1.\n",
    "- Many conceptual mistakes can be made if the three steps are not followed. In particular, attempts at calculating the response distribution through shortcuts are bound to fail.\n",
    "- The PME model is a minimal model. Different forms of decision noise and response noise can be considered as variants and extensions.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "datasci-for-kin",
   "language": "python",
   "name": "datasci-for-kin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
